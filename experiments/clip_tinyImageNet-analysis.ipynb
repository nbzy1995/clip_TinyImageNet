{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook perform some analysis on several CLIP models fine-tuned on TinyImageNet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLaqZVWVR4tE"
   },
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1756051719474,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "NxzIeVi39G-1"
   },
   "outputs": [],
   "source": [
    "LOCAL = True\n",
    "\n",
    "# if run locally:\n",
    "if LOCAL:\n",
    "    DATA_DIR = \"../dataset\"\n",
    "    CODE_DIR = \"../\"\n",
    "# on Colab\n",
    "else:\n",
    "    DATA_DIR = \"/content\"\n",
    "    CODE_DIR = \"./clip_TinyImageNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, We will work under the same dir as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1756051719516,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "z5vVaNV99G-1"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "ROOT = os.path.abspath(CODE_DIR)\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlsOOf4r9G-2"
   },
   "source": [
    "If you want to use Claude Code, uncomment the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1756051719529,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "QnjsVSoia2CU"
   },
   "outputs": [],
   "source": [
    "# !npm install -g @anthropic-ai/claude-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5O1BqBcz9G-2"
   },
   "source": [
    "If use Colab, you need to save output results to google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19655,
     "status": "ok",
     "timestamp": 1756051739176,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "-GlMz92MR4tG",
    "outputId": "bd24bf3e-9aa9-40dc-a620-ee017df8a62f"
   },
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    storage_dir = \"drive/MyDrive/Colab Notebooks/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K23WdGmnfvFA"
   },
   "source": [
    "To copy the code for fine-tuneing clip on tinyImageNet, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1542,
     "status": "ok",
     "timestamp": 1756051740719,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "nTJAmIyofRi9",
    "outputId": "051dd08f-fb95-4c12-8308-3558216f8568"
   },
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    !git clone https://github.com/nbzy1995/clip_TinyImageNet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX1HKoALfFXS"
   },
   "source": [
    "Now we download tiny imagenet dataset. The cell below will create a directory called \"tiny-imagenet-200\" containing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 100958,
     "status": "ok",
     "timestamp": 1756051841679,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "3uz63y5IdKYb"
   },
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    !wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "    !unzip -q tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3HW7EYqfgA8"
   },
   "source": [
    "We now copy pre-computed index for the train/ folder, 90% for training, 10% for validation. The val/ folder will be used as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1756051841715,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "BOwhtbZZdOFf"
   },
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    !cp $CODE_DIR/dataset/tiny_imagenet_train_val_indices.npy /content/tiny_imagenet_train_val_indices.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8bVSnvX9G-4"
   },
   "source": [
    "Python Requirements for fine-tuning clip on tinyImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18766,
     "status": "ok",
     "timestamp": 1756051860490,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "LWGC1rvTR4tH",
    "outputId": "450146f1-9ce5-41cf-e11c-9fec4fd0771c"
   },
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    !pip install --quiet --upgrade pip\n",
    "    !pip install -q -r clip_TinyImageNet/requirements.txt\n",
    "    print(\"✅ Core packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5308,
     "status": "ok",
     "timestamp": 1756051865800,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "oe6uZ6LzR4tH",
    "outputId": "63ef4191-13f6-4153-ed4c-cb7168fb7f86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 System Information:\n",
      "Python version: Python 3.11.5\n",
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n",
      "❌ No GPU available! Please enable GPU runtime in Colab.\n",
      "Runtime > Change runtime type > Hardware accelerator > GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"🔍 System Information:\")\n",
    "print(f\"Python version: {subprocess.check_output(['python', '--version']).decode().strip()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU deivce: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"❌ No GPU available! Please enable GPU runtime in Colab.\")\n",
    "    print(\"Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1756051865820,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "QBl3z7po9G-4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j3zhX-GkTwZ"
   },
   "source": [
    "## Load Models and Dataset\n",
    "Load several models (different fine-tunes), at various epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1756051878622,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "ZS_iRE5Ju4y4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import ModelWrapper,get_model_from_sd, eval_model_on_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63041,
     "status": "ok",
     "timestamp": 1756051989346,
     "user": {
      "displayName": "Yang Zhou",
      "userId": "09717432835810465973"
     },
     "user_tz": 240
    },
    "id": "yNUi-YrmR4tJ",
    "outputId": "e7ac8d53-8c84-4f6d-c353-a3452c725572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoints/config3_6.pt (local)\n",
      "Loading checkpoints/config4_6.pt (local)\n",
      "Loading checkpoints/config5_9.pt (local)\n",
      "Loading checkpoints/config5_9.pt (local)\n",
      "✅ Loaded 3 model_sds successfully!\n",
      "✅ Loaded 3 model_sds successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load all trained model_sds\n",
    "\n",
    "Model_names = ['config1', 'config2', 'config3', 'config4', 'config5']\n",
    "Model_sds = []\n",
    "checkpoint_steps = range(0, 11, 1)\n",
    "\n",
    "base_model, preprocess = clip.load('ViT-B/32', DEVICE, jit=False)\n",
    "base_model = base_model.float() # Force the base model to stay in float32 to match saved weights\n",
    "\n",
    "for name in Model_names:\n",
    "    for checkpoint_step in checkpoint_steps:\n",
    "\n",
    "        model_path = f'checkpoints/{name}_{checkpoint_step}.pt'\n",
    "        drive_path = f'/content/drive/MyDrive/Colab Notebooks/{model_path}'\n",
    "\n",
    "        # Try local first, then Drive backup\n",
    "        sd = None\n",
    "        if os.path.exists(model_path):\n",
    "            print(f'Loading {model_path} (local)')\n",
    "            sd = torch.load(model_path, map_location=DEVICE)\n",
    "        elif os.path.exists(drive_path):\n",
    "            print(f'Loading {drive_path} (from Drive)')\n",
    "            sd = torch.load(drive_path, map_location=DEVICE)\n",
    "        \n",
    "        if sd:\n",
    "            Model_sds.append({\n",
    "                'name': name,\n",
    "                'epoch': checkpoint_step,\n",
    "                'state_dict': sd\n",
    "            })\n",
    "\n",
    "print(f\"✅ Loaded {len(Model_sds)} model_sds successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "✅Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating datasets...\")\n",
    "\n",
    "from dataset.tiny_imagenet import TinyImageNet\n",
    "\n",
    "# Use CLIP's expected preprocessing for proper model evaluation\n",
    "import clip\n",
    "clip_preprocess = clip.load('ViT-B/32', DEVICE, jit=False)[1]\n",
    "\n",
    "data_tinyImageNet = TinyImageNet(\n",
    "    train_preprocess=clip_preprocess,  # Use CLIP preprocessing for training\n",
    "    eval_preprocess=clip_preprocess,   # Use CLIP preprocessing for evaluation\n",
    "    location=DATA_DIR,\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "test_loader = data_tinyImageNet.test_loader\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter space distance - pairs of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairs: At any two config, each at any epoch, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute L2 distance, over all pairs of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Analyzing 3 models\n",
      "Total parameter groups: 160\n",
      "Classification head parameter groups: 2\n",
      "Backbone parameter groups: 158\n",
      "❌ Some backbone parameters differ between models.\n",
      "❌ Some backbone parameters differ between models.\n",
      "✅ DataFrame with L2 distances created.\n",
      "✅ DataFrame with L2 distances created.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model1_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model1_epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "model2_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "model2_epoch",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "l2_distance",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d069567a-c337-4ffc-be3d-58a020c6c475",
       "rows": [
        [
         "0",
         "config3",
         "6",
         "config4",
         "6",
         "6.2661051750183105"
        ],
        [
         "1",
         "config3",
         "6",
         "config5",
         "9",
         "1.0974868535995483"
        ],
        [
         "2",
         "config4",
         "6",
         "config5",
         "9",
         "6.281423091888428"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model1_name</th>\n",
       "      <th>model1_epoch</th>\n",
       "      <th>model2_name</th>\n",
       "      <th>model2_epoch</th>\n",
       "      <th>l2_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>config3</td>\n",
       "      <td>6</td>\n",
       "      <td>config4</td>\n",
       "      <td>6</td>\n",
       "      <td>6.266105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>config3</td>\n",
       "      <td>6</td>\n",
       "      <td>config5</td>\n",
       "      <td>9</td>\n",
       "      <td>1.097487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>config4</td>\n",
       "      <td>6</td>\n",
       "      <td>config5</td>\n",
       "      <td>9</td>\n",
       "      <td>6.281423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model1_name  model1_epoch model2_name  model2_epoch  l2_distance\n",
       "0     config3             6     config4             6     6.266105\n",
       "1     config3             6     config5             9     1.097487\n",
       "2     config4             6     config5             9     6.281423"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_models = len(Model_sds)\n",
    "print(f\"🔍 Analyzing {n_models} models\")\n",
    "\n",
    "# Get all parameter group names from first model\n",
    "param_names = list(Model_sds[0]['state_dict'].keys())\n",
    "print(f\"Total parameter groups: {len(param_names)}\")\n",
    "\n",
    "# Separate classification head vs backbone parameters\n",
    "head_params = [k for k in param_names if k.startswith('classification_head')]\n",
    "backbone_params = [k for k in param_names if not k.startswith('classification_head')]\n",
    "\n",
    "print(f\"Classification head parameter groups: {len(head_params)}\")\n",
    "print(f\"Backbone parameter groups: {len(backbone_params)}\")\n",
    "\n",
    "# Check if backbone parameters are identical\n",
    "backbone_identical = True\n",
    "for param_name in backbone_params:\n",
    "    ref_param = Model_sds[0]['state_dict'][param_name]\n",
    "    for i in range(1, n_models):\n",
    "        if not torch.allclose(ref_param, Model_sds[i]['state_dict'][param_name], atol=1e-6):\n",
    "            # print(f\"❌ {param_name} differs between model 0 and model {i}\")\n",
    "            backbone_identical = False\n",
    "            break\n",
    "\n",
    "if backbone_identical:\n",
    "    print(\"✅ All backbone parameters are identical across models!\")\n",
    "else:\n",
    "    print(\"❌ Some backbone parameters differ between models.\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    for j in range(i + 1, n_models):\n",
    "        model_i = Model_sds[i]\n",
    "        model_j = Model_sds[j]\n",
    "\n",
    "        # Flatten and concatenate all parameters\n",
    "        params_i = torch.cat([p.flatten() for p in model_i['state_dict'].values()])\n",
    "        params_j = torch.cat([p.flatten() for p in model_j['state_dict'].values()])\n",
    "\n",
    "        # Compute L2 distance\n",
    "        l2_dist = torch.norm(params_i - params_j).item()\n",
    "\n",
    "        records.append({\n",
    "            'model1_name': model_i['name'],\n",
    "            'model1_epoch': model_i['epoch'],\n",
    "            'model2_name': model_j['name'],\n",
    "            'model2_epoch': model_j['epoch'],\n",
    "            'l2_distance': l2_dist,\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(records)\n",
    "print(\"✅ DataFrame with L2 distances created.\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: distance(epoch)\n",
    "L2 distance between any two different models at same epoch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model soup performance (on test split) -  pairs of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(state_di, weights=None):\n",
    "    \"\"\"Create a model soup by averaging state dicts with given weights\"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0 / len(state_di)] * len(state_di)\n",
    "\n",
    "    # Start with the first model weighted\n",
    "    soup_state_dict = {k: v.clone() * weights[0] for k, v in state_di[0].items()}\n",
    "\n",
    "    # Add remaining models\n",
    "    for i, state_dict in enumerate(state_di[1:], 1):\n",
    "        for k, v in state_dict.items():\n",
    "            soup_state_dict[k] += v.clone() * weights[i]\n",
    "\n",
    "    return soup_state_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute soup accuracy for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yang/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0% 0/1250]\tAcc: 62.50\tData (t) 3.840\tBatch (t) 3.975\n",
      "[2% 20/1250]\tAcc: 80.36\tData (t) 0.000\tBatch (t) 0.099\n",
      "[3% 40/1250]\tAcc: 78.35\tData (t) 0.000\tBatch (t) 0.103\n",
      "[5% 60/1250]\tAcc: 80.53\tData (t) 0.000\tBatch (t) 0.104\n",
      "[6% 80/1250]\tAcc: 81.33\tData (t) 0.000\tBatch (t) 0.110\n",
      "[8% 100/1250]\tAcc: 81.06\tData (t) 0.000\tBatch (t) 0.114\n",
      "[10% 120/1250]\tAcc: 80.99\tData (t) 0.000\tBatch (t) 0.115\n",
      "[11% 140/1250]\tAcc: 81.12\tData (t) 0.000\tBatch (t) 0.114\n",
      "[13% 160/1250]\tAcc: 81.83\tData (t) 0.000\tBatch (t) 0.109\n",
      "[14% 180/1250]\tAcc: 81.91\tData (t) 0.000\tBatch (t) 0.115\n",
      "[16% 200/1250]\tAcc: 82.21\tData (t) 0.001\tBatch (t) 0.117\n",
      "[18% 220/1250]\tAcc: 82.30\tData (t) 0.000\tBatch (t) 0.109\n",
      "[19% 240/1250]\tAcc: 82.16\tData (t) 0.001\tBatch (t) 0.109\n",
      "[21% 260/1250]\tAcc: 82.52\tData (t) 0.001\tBatch (t) 0.124\n",
      "[22% 280/1250]\tAcc: 83.10\tData (t) 0.000\tBatch (t) 0.133\n",
      "[24% 300/1250]\tAcc: 83.35\tData (t) 0.000\tBatch (t) 0.129\n",
      "[26% 320/1250]\tAcc: 83.49\tData (t) 0.001\tBatch (t) 0.109\n",
      "[27% 340/1250]\tAcc: 83.28\tData (t) 0.000\tBatch (t) 0.110\n",
      "[29% 360/1250]\tAcc: 83.34\tData (t) 0.000\tBatch (t) 0.110\n",
      "[30% 380/1250]\tAcc: 83.27\tData (t) 0.000\tBatch (t) 0.111\n",
      "[32% 400/1250]\tAcc: 83.45\tData (t) 0.000\tBatch (t) 0.111\n",
      "[34% 420/1250]\tAcc: 83.64\tData (t) 0.000\tBatch (t) 0.108\n",
      "[35% 440/1250]\tAcc: 83.82\tData (t) 0.000\tBatch (t) 0.110\n",
      "[37% 460/1250]\tAcc: 83.84\tData (t) 0.000\tBatch (t) 0.113\n",
      "[38% 480/1250]\tAcc: 83.84\tData (t) 0.004\tBatch (t) 0.150\n",
      "[40% 500/1250]\tAcc: 83.76\tData (t) 0.000\tBatch (t) 0.117\n",
      "[42% 520/1250]\tAcc: 83.85\tData (t) 0.000\tBatch (t) 0.113\n",
      "[43% 540/1250]\tAcc: 83.80\tData (t) 0.001\tBatch (t) 0.128\n",
      "[45% 560/1250]\tAcc: 83.91\tData (t) 0.001\tBatch (t) 0.113\n",
      "[46% 580/1250]\tAcc: 83.93\tData (t) 0.000\tBatch (t) 0.109\n",
      "[48% 600/1250]\tAcc: 84.01\tData (t) 0.001\tBatch (t) 0.109\n",
      "[50% 620/1250]\tAcc: 83.90\tData (t) 0.000\tBatch (t) 0.112\n",
      "[51% 640/1250]\tAcc: 83.81\tData (t) 0.001\tBatch (t) 0.111\n",
      "[53% 660/1250]\tAcc: 83.72\tData (t) 0.001\tBatch (t) 0.112\n",
      "[54% 680/1250]\tAcc: 83.72\tData (t) 0.000\tBatch (t) 0.113\n",
      "[56% 700/1250]\tAcc: 83.65\tData (t) 0.001\tBatch (t) 0.127\n",
      "[58% 720/1250]\tAcc: 83.70\tData (t) 0.000\tBatch (t) 0.112\n",
      "[59% 740/1250]\tAcc: 83.81\tData (t) 0.000\tBatch (t) 0.118\n",
      "[61% 760/1250]\tAcc: 83.84\tData (t) 0.000\tBatch (t) 0.125\n",
      "[62% 780/1250]\tAcc: 83.79\tData (t) 0.001\tBatch (t) 0.196\n",
      "[64% 800/1250]\tAcc: 83.69\tData (t) 0.001\tBatch (t) 0.141\n",
      "[66% 820/1250]\tAcc: 83.69\tData (t) 0.001\tBatch (t) 0.153\n",
      "[67% 840/1250]\tAcc: 83.70\tData (t) 0.001\tBatch (t) 0.136\n",
      "[69% 860/1250]\tAcc: 83.74\tData (t) 0.000\tBatch (t) 0.113\n",
      "[70% 880/1250]\tAcc: 83.73\tData (t) 0.000\tBatch (t) 0.118\n",
      "[72% 900/1250]\tAcc: 83.84\tData (t) 0.001\tBatch (t) 0.265\n",
      "[74% 920/1250]\tAcc: 83.79\tData (t) 0.001\tBatch (t) 0.118\n",
      "[75% 940/1250]\tAcc: 83.75\tData (t) 0.001\tBatch (t) 0.117\n",
      "[77% 960/1250]\tAcc: 83.74\tData (t) 0.001\tBatch (t) 0.131\n",
      "[78% 980/1250]\tAcc: 83.70\tData (t) 0.000\tBatch (t) 0.161\n",
      "[80% 1000/1250]\tAcc: 83.68\tData (t) 0.000\tBatch (t) 0.127\n",
      "[82% 1020/1250]\tAcc: 83.66\tData (t) 0.001\tBatch (t) 0.121\n",
      "[83% 1040/1250]\tAcc: 83.55\tData (t) 0.000\tBatch (t) 0.115\n",
      "[85% 1060/1250]\tAcc: 83.48\tData (t) 0.001\tBatch (t) 0.113\n",
      "[86% 1080/1250]\tAcc: 83.53\tData (t) 0.000\tBatch (t) 0.131\n",
      "[88% 1100/1250]\tAcc: 83.58\tData (t) 0.001\tBatch (t) 0.133\n",
      "[90% 1120/1250]\tAcc: 83.60\tData (t) 0.001\tBatch (t) 0.141\n",
      "[91% 1140/1250]\tAcc: 83.61\tData (t) 0.001\tBatch (t) 0.134\n",
      "[93% 1160/1250]\tAcc: 83.63\tData (t) 0.001\tBatch (t) 0.134\n",
      "[94% 1180/1250]\tAcc: 83.58\tData (t) 0.000\tBatch (t) 0.129\n",
      "[96% 1200/1250]\tAcc: 83.64\tData (t) 0.000\tBatch (t) 0.147\n",
      "[98% 1220/1250]\tAcc: 83.66\tData (t) 0.000\tBatch (t) 0.194\n",
      "[99% 1240/1250]\tAcc: 83.71\tData (t) 0.000\tBatch (t) 0.136\n",
      "[0% 0/1250]\tAcc: 50.00\tData (t) 3.403\tBatch (t) 3.550\n",
      "[2% 20/1250]\tAcc: 78.57\tData (t) 0.000\tBatch (t) 0.130\n",
      "[3% 40/1250]\tAcc: 78.05\tData (t) 0.000\tBatch (t) 0.155\n",
      "[5% 60/1250]\tAcc: 77.46\tData (t) 0.000\tBatch (t) 0.145\n",
      "[6% 80/1250]\tAcc: 77.62\tData (t) 0.001\tBatch (t) 0.150\n",
      "[8% 100/1250]\tAcc: 77.60\tData (t) 0.000\tBatch (t) 0.134\n",
      "[10% 120/1250]\tAcc: 77.48\tData (t) 0.001\tBatch (t) 0.136\n",
      "[11% 140/1250]\tAcc: 77.39\tData (t) 0.000\tBatch (t) 0.130\n",
      "[13% 160/1250]\tAcc: 77.87\tData (t) 0.001\tBatch (t) 0.135\n",
      "[14% 180/1250]\tAcc: 78.25\tData (t) 0.000\tBatch (t) 0.134\n",
      "[16% 200/1250]\tAcc: 78.11\tData (t) 0.001\tBatch (t) 0.135\n",
      "[18% 220/1250]\tAcc: 78.11\tData (t) 0.001\tBatch (t) 0.136\n",
      "[19% 240/1250]\tAcc: 78.27\tData (t) 0.000\tBatch (t) 0.131\n",
      "[21% 260/1250]\tAcc: 78.50\tData (t) 0.001\tBatch (t) 0.136\n",
      "[22% 280/1250]\tAcc: 78.78\tData (t) 0.000\tBatch (t) 0.134\n",
      "[24% 300/1250]\tAcc: 79.32\tData (t) 0.000\tBatch (t) 0.132\n",
      "[26% 320/1250]\tAcc: 79.44\tData (t) 0.001\tBatch (t) 0.134\n",
      "[27% 340/1250]\tAcc: 79.40\tData (t) 0.001\tBatch (t) 0.132\n",
      "[29% 360/1250]\tAcc: 79.26\tData (t) 0.001\tBatch (t) 0.141\n",
      "[30% 380/1250]\tAcc: 79.27\tData (t) 0.001\tBatch (t) 0.134\n",
      "[32% 400/1250]\tAcc: 79.43\tData (t) 0.001\tBatch (t) 0.138\n",
      "[34% 420/1250]\tAcc: 79.54\tData (t) 0.001\tBatch (t) 0.134\n",
      "[35% 440/1250]\tAcc: 79.76\tData (t) 0.000\tBatch (t) 0.138\n",
      "[37% 460/1250]\tAcc: 79.66\tData (t) 0.001\tBatch (t) 0.141\n",
      "[38% 480/1250]\tAcc: 79.73\tData (t) 0.000\tBatch (t) 0.195\n",
      "[40% 500/1250]\tAcc: 79.84\tData (t) 0.001\tBatch (t) 0.130\n",
      "[42% 520/1250]\tAcc: 79.89\tData (t) 0.000\tBatch (t) 0.135\n",
      "[43% 540/1250]\tAcc: 79.78\tData (t) 0.000\tBatch (t) 0.133\n",
      "[45% 560/1250]\tAcc: 79.90\tData (t) 0.001\tBatch (t) 0.134\n",
      "[46% 580/1250]\tAcc: 79.86\tData (t) 0.000\tBatch (t) 0.133\n",
      "[48% 600/1250]\tAcc: 79.93\tData (t) 0.001\tBatch (t) 0.137\n",
      "[50% 620/1250]\tAcc: 79.83\tData (t) 0.001\tBatch (t) 0.137\n",
      "[51% 640/1250]\tAcc: 79.66\tData (t) 0.000\tBatch (t) 0.133\n",
      "[53% 660/1250]\tAcc: 79.61\tData (t) 0.000\tBatch (t) 0.134\n",
      "[54% 680/1250]\tAcc: 79.66\tData (t) 0.000\tBatch (t) 0.134\n",
      "[56% 700/1250]\tAcc: 79.67\tData (t) 0.000\tBatch (t) 0.139\n",
      "[58% 720/1250]\tAcc: 79.70\tData (t) 0.001\tBatch (t) 0.139\n",
      "[59% 740/1250]\tAcc: 79.77\tData (t) 0.001\tBatch (t) 0.141\n",
      "[61% 760/1250]\tAcc: 79.68\tData (t) 0.000\tBatch (t) 0.211\n",
      "[62% 780/1250]\tAcc: 79.67\tData (t) 0.000\tBatch (t) 0.143\n",
      "[64% 800/1250]\tAcc: 79.59\tData (t) 0.001\tBatch (t) 0.127\n",
      "[66% 820/1250]\tAcc: 79.58\tData (t) 0.001\tBatch (t) 0.137\n",
      "[67% 840/1250]\tAcc: 79.58\tData (t) 0.000\tBatch (t) 0.134\n",
      "[69% 860/1250]\tAcc: 79.53\tData (t) 0.000\tBatch (t) 0.139\n",
      "[70% 880/1250]\tAcc: 79.55\tData (t) 0.001\tBatch (t) 0.135\n",
      "[72% 900/1250]\tAcc: 79.55\tData (t) 0.000\tBatch (t) 0.132\n",
      "[74% 920/1250]\tAcc: 79.59\tData (t) 0.000\tBatch (t) 0.132\n",
      "[75% 940/1250]\tAcc: 79.57\tData (t) 0.000\tBatch (t) 0.135\n",
      "[77% 960/1250]\tAcc: 79.50\tData (t) 0.000\tBatch (t) 0.131\n",
      "[78% 980/1250]\tAcc: 79.59\tData (t) 0.001\tBatch (t) 0.132\n",
      "[80% 1000/1250]\tAcc: 79.56\tData (t) 0.000\tBatch (t) 0.130\n",
      "[82% 1020/1250]\tAcc: 79.58\tData (t) 0.000\tBatch (t) 0.143\n",
      "[83% 1040/1250]\tAcc: 79.45\tData (t) 0.001\tBatch (t) 0.136\n",
      "[85% 1060/1250]\tAcc: 79.48\tData (t) 0.001\tBatch (t) 0.131\n",
      "[86% 1080/1250]\tAcc: 79.51\tData (t) 0.001\tBatch (t) 0.130\n",
      "[88% 1100/1250]\tAcc: 79.50\tData (t) 0.001\tBatch (t) 0.136\n",
      "[90% 1120/1250]\tAcc: 79.50\tData (t) 0.000\tBatch (t) 0.146\n",
      "[91% 1140/1250]\tAcc: 79.47\tData (t) 0.000\tBatch (t) 0.149\n",
      "[93% 1160/1250]\tAcc: 79.50\tData (t) 0.000\tBatch (t) 0.135\n",
      "[94% 1180/1250]\tAcc: 79.43\tData (t) 0.000\tBatch (t) 0.137\n",
      "[96% 1200/1250]\tAcc: 79.50\tData (t) 0.001\tBatch (t) 0.131\n",
      "[98% 1220/1250]\tAcc: 79.51\tData (t) 0.001\tBatch (t) 0.179\n",
      "[99% 1240/1250]\tAcc: 79.56\tData (t) 0.001\tBatch (t) 0.132\n",
      "[0% 0/1250]\tAcc: 75.00\tData (t) 3.900\tBatch (t) 4.214\n",
      "[2% 20/1250]\tAcc: 80.36\tData (t) 0.001\tBatch (t) 0.137\n",
      "[3% 40/1250]\tAcc: 78.35\tData (t) 0.001\tBatch (t) 0.136\n",
      "[5% 60/1250]\tAcc: 80.12\tData (t) 0.001\tBatch (t) 0.137\n",
      "[6% 80/1250]\tAcc: 80.40\tData (t) 0.001\tBatch (t) 0.140\n",
      "[8% 100/1250]\tAcc: 80.45\tData (t) 0.000\tBatch (t) 0.131\n",
      "[10% 120/1250]\tAcc: 80.37\tData (t) 0.000\tBatch (t) 0.131\n",
      "[11% 140/1250]\tAcc: 80.50\tData (t) 0.001\tBatch (t) 0.129\n",
      "[13% 160/1250]\tAcc: 81.52\tData (t) 0.001\tBatch (t) 0.132\n",
      "[14% 180/1250]\tAcc: 81.63\tData (t) 0.001\tBatch (t) 0.134\n",
      "[16% 200/1250]\tAcc: 81.90\tData (t) 0.001\tBatch (t) 0.141\n",
      "[18% 220/1250]\tAcc: 81.96\tData (t) 0.001\tBatch (t) 0.137\n",
      "[19% 240/1250]\tAcc: 81.74\tData (t) 0.001\tBatch (t) 0.195\n",
      "[21% 260/1250]\tAcc: 82.09\tData (t) 0.001\tBatch (t) 0.137\n",
      "[22% 280/1250]\tAcc: 82.70\tData (t) 0.000\tBatch (t) 0.143\n",
      "[24% 300/1250]\tAcc: 83.06\tData (t) 0.001\tBatch (t) 0.148\n",
      "[26% 320/1250]\tAcc: 83.18\tData (t) 0.000\tBatch (t) 0.133\n",
      "[27% 340/1250]\tAcc: 83.03\tData (t) 0.000\tBatch (t) 0.136\n",
      "[29% 360/1250]\tAcc: 83.03\tData (t) 0.001\tBatch (t) 0.132\n",
      "[30% 380/1250]\tAcc: 82.91\tData (t) 0.001\tBatch (t) 0.120\n",
      "[32% 400/1250]\tAcc: 83.10\tData (t) 0.000\tBatch (t) 0.131\n",
      "[34% 420/1250]\tAcc: 83.22\tData (t) 0.000\tBatch (t) 0.135\n",
      "[35% 440/1250]\tAcc: 83.39\tData (t) 0.000\tBatch (t) 0.134\n",
      "[37% 460/1250]\tAcc: 83.41\tData (t) 0.000\tBatch (t) 0.136\n",
      "[38% 480/1250]\tAcc: 83.39\tData (t) 0.001\tBatch (t) 0.136\n",
      "[40% 500/1250]\tAcc: 83.36\tData (t) 0.001\tBatch (t) 0.133\n",
      "[42% 520/1250]\tAcc: 83.52\tData (t) 0.000\tBatch (t) 0.138\n",
      "[43% 540/1250]\tAcc: 83.43\tData (t) 0.001\tBatch (t) 0.139\n",
      "[45% 560/1250]\tAcc: 83.58\tData (t) 0.000\tBatch (t) 0.146\n",
      "[46% 580/1250]\tAcc: 83.67\tData (t) 0.000\tBatch (t) 0.145\n",
      "[48% 600/1250]\tAcc: 83.80\tData (t) 0.000\tBatch (t) 0.135\n",
      "[50% 620/1250]\tAcc: 83.70\tData (t) 0.000\tBatch (t) 0.132\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, n_models):\n\u001b[32m      4\u001b[39m     model = get_model_from_sd(soup_model_sd[i,j], base_model)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     Results_test_acc[i,j] = \u001b[43meval_model_on_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     Results_test_acc[j,i] = Results_test_acc[i,j]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/utils.py:75\u001b[39m, in \u001b[36meval_model_on_dataset\u001b[39m\u001b[34m(model, dataloader)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mimage_paths\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[32m     73\u001b[39m     image_paths = batch[\u001b[33m'\u001b[39m\u001b[33mimage_paths\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     78\u001b[39m     logits = logits[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/utils.py:24\u001b[39m, in \u001b[36mModelWrapper.forward\u001b[39m\u001b[34m(self, images, return_features)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, return_features=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.normalize:\n\u001b[32m     26\u001b[39m         features = features / features.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/clip/model.py:341\u001b[39m, in \u001b[36mCLIP.encode_image\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/clip/model.py:232\u001b[39m, in \u001b[36mVisionTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    229\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_pre(x)\n\u001b[32m    231\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[32m    235\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_post(x[:, \u001b[32m0\u001b[39m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/clip/model.py:203\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/clip/model.py:190\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.ln_2(x))\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/clip/model.py:187\u001b[39m, in \u001b[36mResidualAttentionBlock.attention\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mattention\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m.attn_mask = \u001b[38;5;28mself\u001b[39m.attn_mask.to(dtype=x.dtype, device=x.device) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1380\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1354\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1355\u001b[39m         query,\n\u001b[32m   1356\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1377\u001b[39m         is_causal=is_causal,\n\u001b[32m   1378\u001b[39m     )\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/model-merge/model-soups/clip_TinyImageNet/.venv/lib/python3.11/site-packages/torch/nn/functional.py:6491\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6484\u001b[39m attn_output = scaled_dot_product_attention(\n\u001b[32m   6485\u001b[39m     q, k, v, attn_mask, dropout_p, is_causal\n\u001b[32m   6486\u001b[39m )\n\u001b[32m   6487\u001b[39m attn_output = (\n\u001b[32m   6488\u001b[39m     attn_output.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m).contiguous().view(bsz * tgt_len, embed_dim)\n\u001b[32m   6489\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m6491\u001b[39m attn_output = \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_proj_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6492\u001b[39m attn_output = attn_output.view(tgt_len, bsz, attn_output.size(\u001b[32m1\u001b[39m))\n\u001b[32m   6493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   6494\u001b[39m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "soup_accuracies = []\n",
    "# Create a dictionary for quick state_dict lookup\n",
    "model_map = {(m['name'], m['epoch']): m['state_dict'] for m in Model_sds}\n",
    "\n",
    "for _, row in tqdm(df_results.iterrows(), total=len(df_results), desc=\"Evaluating Soups\"):\n",
    "    sd1 = model_map[(row['model1_name'], row['model1_epoch'])]\n",
    "    sd2 = model_map[(row['model2_name'], row['model2_epoch'])]\n",
    "    \n",
    "    # Create a soup model:  theta = 1/2(theta_i + theta_j)\n",
    "    soup_sd = create_soup([sd1, sd2])\n",
    "    \n",
    "    # Evaluate the soup on test split\n",
    "    soup_model = get_model_from_sd(soup_sd, base_model)\n",
    "    acc = eval_model_on_dataset(soup_model, test_loader)\n",
    "    soup_accuracies.append(acc)\n",
    "\n",
    "df_results['soup_accuracy'] = soup_accuracies\n",
    "print(\"✅ Soup accuracies calculated and added to DataFrame.\")\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: soup accuracy(epoch)\n",
    "soup accuracy for any two models at same epoch.\n",
    "\n",
    "soup improvement above mean of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between Soup performance and model L2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot: soup improvement vs L2 distance\n",
    "scatters"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b_-KAJBYR4tH",
    "TmkIYFQuR4tI"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
