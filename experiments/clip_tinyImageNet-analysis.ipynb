{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC9TTG4sZv6y"
      },
      "source": [
        "This notebook perform some analysis on several CLIP models fine-tuned on TinyImageNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLaqZVWVR4tE"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NxzIeVi39G-1"
      },
      "outputs": [],
      "source": [
        "LOCAL = False\n",
        "\n",
        "# if run locally:\n",
        "if LOCAL:\n",
        "    DATA_DIR = \"../dataset\"\n",
        "    CODE_DIR = \"../\"\n",
        "# on Colab\n",
        "else:\n",
        "    DATA_DIR = \"/content\"\n",
        "    CODE_DIR = \"./clip_TinyImageNet\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuQWHgduZv60"
      },
      "source": [
        "By default, We will work under the same dir as this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z5vVaNV99G-1"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "ROOT = os.path.abspath(CODE_DIR)\n",
        "if ROOT not in sys.path:\n",
        "    sys.path.insert(0, ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlsOOf4r9G-2"
      },
      "source": [
        "If you want to use Claude Code, uncomment the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QnjsVSoia2CU"
      },
      "outputs": [],
      "source": [
        "# !npm install -g @anthropic-ai/claude-code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O1BqBcz9G-2"
      },
      "source": [
        "If use Colab, you need to save output results to google drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GlMz92MR4tG",
        "outputId": "35e67c45-8521-4c7f-ba3c-46ad1efd01a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if not LOCAL:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    storage_dir = \"drive/MyDrive/research-model_merge/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K23WdGmnfvFA"
      },
      "source": [
        "To copy the code for fine-tuneing clip on tinyImageNet, run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTJAmIyofRi9",
        "outputId": "837cd4ab-e33b-49e5-a62b-9ce1998a480e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clip_TinyImageNet'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 95 (delta 47), reused 91 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (95/95), 2.65 MiB | 17.41 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "if not LOCAL:\n",
        "    !git clone https://github.com/nbzy1995/clip_TinyImageNet.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX1HKoALfFXS"
      },
      "source": [
        "Now we download tiny imagenet dataset. The cell below will create a directory called \"tiny-imagenet-200\" containing the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3uz63y5IdKYb"
      },
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    if not os.path.exists(\"tiny-imagenet-200\"):\n",
        "      !wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "      !unzip -q tiny-imagenet-200.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3HW7EYqfgA8"
      },
      "source": [
        "We now copy pre-computed index for the train/ folder, 90% for training, 10% for validation. The val/ folder will be used as test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BOwhtbZZdOFf"
      },
      "outputs": [],
      "source": [
        "if not LOCAL:\n",
        "    !cp $CODE_DIR/dataset/tiny_imagenet_train_val_indices.npy /content/tiny_imagenet_train_val_indices.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8bVSnvX9G-4"
      },
      "source": [
        "Python Requirements for fine-tuning clip on tinyImageNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWGC1rvTR4tH",
        "outputId": "870dadda-8e80-461c-cefd-5cc522449b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.8 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'clip' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'clip'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "✅ Core packages installed!\n"
          ]
        }
      ],
      "source": [
        "if not LOCAL:\n",
        "    !pip install --quiet --upgrade pip\n",
        "    !pip install -q -r clip_TinyImageNet/requirements.txt\n",
        "    print(\"✅ Core packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKklR4B4Zv62"
      },
      "source": [
        "Device Info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe6uZ6LzR4tH",
        "outputId": "5a02c391-8b60-42ef-958b-b1e78402c8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 System Information:\n",
            "Python version: Python 3.12.11\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU deivce: Tesla T4\n",
            "GPU memory: 15.8 GB\n",
            "CUDA version: 12.6\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(\"🔍 System Information:\")\n",
        "print(f\"Python version: {subprocess.check_output(['python', '--version']).decode().strip()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU deivce: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"❌ No GPU available! Please enable GPU runtime in Colab.\")\n",
        "    print(\"Runtime > Change runtime type > Hardware accelerator > GPU\")\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QBl3z7po9G-4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j3zhX-GkTwZ"
      },
      "source": [
        "## Load Models and Dataset\n",
        "Load several models (different fine-tunes), at various epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZS_iRE5Ju4y4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "from utils import ModelWrapper,get_model_from_sd, eval_model_on_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "yNUi-YrmR4tJ",
        "outputId": "5e9509b1-875a-48b9-8b4c-e7ea65f4b162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:17<00:00, 20.3MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading drive/MyDrive/research-model_merge//checkpoints/config1_0.pt (from Drive)\n",
            "Loading drive/MyDrive/research-model_merge//checkpoints/config1_2.pt (from Drive)\n",
            "Loading drive/MyDrive/research-model_merge//checkpoints/config1_4.pt (from Drive)\n",
            "Loading drive/MyDrive/research-model_merge//checkpoints/config1_6.pt (from Drive)\n",
            "Loading drive/MyDrive/research-model_merge//checkpoints/config1_8.pt (from Drive)\n",
            "Loading drive/MyDrive/research-model_merge//checkpoints/config1_10.pt (from Drive)\n",
            "Loading drive/MyDrive/research-model_merge//checkpoints/config2_0.pt (from Drive)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1153721647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Loading {drive_path} (from Drive)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrive_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2117\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2118\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2119\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2120\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m                         \u001b[0;34mf\"Only persistent_load of storage is allowed, but got {pid[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                     )\n\u001b[0;32m--> 532\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_BINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2081\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   2084\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2034\u001b[0m                     )\n\u001b[1;32m   2035\u001b[0m             storage = (\n\u001b[0;32m-> 2036\u001b[0;31m                 \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2037\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load all trained model_sds\n",
        "\n",
        "Model_names = ['config1', 'config2', 'config3', 'config4', 'config5']\n",
        "Model_sds = []\n",
        "checkpoint_steps = range(0, 11, 2)\n",
        "\n",
        "Base_model, preprocess = clip.load('ViT-B/32', DEVICE, jit=False)\n",
        "Base_model = Base_model.float() # Force the base model to stay in float32 to match saved weights\n",
        "\n",
        "for name in Model_names:\n",
        "    for checkpoint_step in checkpoint_steps:\n",
        "\n",
        "        model_path = f'checkpoints/{name}_{checkpoint_step}.pt'\n",
        "        drive_path = f'{storage_dir}/{model_path}'\n",
        "\n",
        "        # Try local first, then Drive backup\n",
        "        sd = None\n",
        "        if os.path.exists(model_path):\n",
        "            print(f'Loading {model_path} (local)')\n",
        "            sd = torch.load(model_path, map_location=DEVICE)\n",
        "        elif os.path.exists(drive_path):\n",
        "            print(f'Loading {drive_path} (from Drive)')\n",
        "            sd = torch.load(drive_path, map_location=DEVICE)\n",
        "\n",
        "        if sd:\n",
        "            Model_sds.append({\n",
        "                'name': name,\n",
        "                'epoch': checkpoint_step,\n",
        "                'state_dict': sd\n",
        "            })\n",
        "\n",
        "print(f\"✅ Loaded {len(Model_sds)} model_sds successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "940P4CWPZv64"
      },
      "outputs": [],
      "source": [
        "print(\"Creating datasets...\")\n",
        "\n",
        "from dataset.tiny_imagenet import TinyImageNet\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Use CLIP's expected preprocessing for proper model evaluation\n",
        "import clip\n",
        "clip_preprocess = clip.load('ViT-B/32', DEVICE, jit=False)[1]\n",
        "\n",
        "data_tinyImageNet = TinyImageNet(\n",
        "    train_preprocess=clip_preprocess,  # Use CLIP preprocessing for training\n",
        "    eval_preprocess=clip_preprocess,   # Use CLIP preprocessing for evaluation\n",
        "    location=DATA_DIR,\n",
        "    batch_size=32,\n",
        "    num_workers=2,\n",
        "    distributed=False,\n",
        ")\n",
        "\n",
        "# For local testing, let's use a small subset of the test data\n",
        "if LOCAL:\n",
        "    subset_fraction = 0.05 # Use 5% of the test data\n",
        "    num_samples = int(len(data_tinyImageNet.test_dataset) * subset_fraction)\n",
        "    subset_indices = list(range(num_samples))\n",
        "    test_subset = Subset(data_tinyImageNet.test_dataset, subset_indices)\n",
        "\n",
        "    Test_loader = torch.utils.data.DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=data_tinyImageNet.batch_size,\n",
        "        num_workers=data_tinyImageNet.num_workers\n",
        "    )\n",
        "    print(f\"🧪 Using a subset of the test data for local run: {len(test_subset)} samples.\")\n",
        "else:\n",
        "    Test_loader = data_tinyImageNet.test_loader\n",
        "    print(\"✅ Using the full test dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTGgczvNZv64"
      },
      "source": [
        "## Parameter space distance - pairs of models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2-venFaZv64"
      },
      "source": [
        "Pairs: At any two config, each at any epoch,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukELoerEZv64"
      },
      "source": [
        "Compute L2 distance, over all pairs of models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgjczSxXZv64"
      },
      "outputs": [],
      "source": [
        "n_models = len(Model_sds)\n",
        "print(f\"🔍 Analyzing {n_models} models\")\n",
        "\n",
        "# Get all parameter group names from first model\n",
        "param_names = list(Model_sds[0]['state_dict'].keys())\n",
        "print(f\"Total parameter groups: {len(param_names)}\")\n",
        "\n",
        "# Separate classification head vs backbone parameters\n",
        "head_params = [k for k in param_names if k.startswith('classification_head')]\n",
        "backbone_params = [k for k in param_names if not k.startswith('classification_head')]\n",
        "\n",
        "print(f\"Classification head parameter groups: {len(head_params)}\")\n",
        "print(f\"Backbone parameter groups: {len(backbone_params)}\")\n",
        "\n",
        "# Check if backbone parameters are identical\n",
        "backbone_identical = True\n",
        "for param_name in backbone_params:\n",
        "    ref_param = Model_sds[0]['state_dict'][param_name]\n",
        "    for i in range(1, n_models):\n",
        "        if not torch.allclose(ref_param, Model_sds[i]['state_dict'][param_name], atol=1e-6):\n",
        "            # print(f\"❌ {param_name} differs between model 0 and model {i}\")\n",
        "            backbone_identical = False\n",
        "            break\n",
        "\n",
        "if backbone_identical:\n",
        "    print(\"✅ All backbone parameters are identical across models!\")\n",
        "else:\n",
        "    print(\"❌ Some backbone parameters differ between models.\")\n",
        "\n",
        "records = []\n",
        "\n",
        "for i in range(n_models):\n",
        "    for j in range(i + 1, n_models):\n",
        "        model_i = Model_sds[i]\n",
        "        model_j = Model_sds[j]\n",
        "\n",
        "        # Flatten and concatenate all parameters\n",
        "        params_i = torch.cat([p.flatten() for p in model_i['state_dict'].values()])\n",
        "        params_j = torch.cat([p.flatten() for p in model_j['state_dict'].values()])\n",
        "\n",
        "        # Compute L2 distance\n",
        "        l2_dist = torch.norm(params_i - params_j).item()\n",
        "\n",
        "        records.append({\n",
        "            'model1_name': model_i['name'],\n",
        "            'model1_epoch': model_i['epoch'],\n",
        "            'model2_name': model_j['name'],\n",
        "            'model2_epoch': model_j['epoch'],\n",
        "            'l2_distance': l2_dist,\n",
        "        })\n",
        "\n",
        "# Create DataFrame\n",
        "df_results = pd.DataFrame(records)\n",
        "print(\"✅ DataFrame with L2 distances created.\")\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4DnUTAvZv64"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_CtvIhRZv64"
      },
      "source": [
        "### Plot: distance(epoch)\n",
        "L2 distance between any two different models at same epoch during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boTU0xpEZv64"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter for pairs of different models at the same epoch\n",
        "same_epoch_df = df_results[df_results['model1_epoch'] == df_results['model2_epoch']].copy()\n",
        "same_epoch_df['model_pair'] = same_epoch_df.apply(lambda r: f\"{r['model1_name']} vs {r['model2_name']}\", axis=1)\n",
        "\n",
        "# Create the plot\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "sns.lineplot(\n",
        "    data=same_epoch_df,\n",
        "    x='model1_epoch',\n",
        "    y='l2_distance',\n",
        "    hue='model_pair',\n",
        "    marker='o',\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "ax.set_title('L2 Distance Between Model Pairs at Each Epoch', fontsize=16)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('L2 Distance', fontsize=12)\n",
        "ax.legend(title='Model Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(f'{storage_dir}/pair_distance_per_epoch.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV8ZW2NCZv64"
      },
      "source": [
        "## Model soup performance (on test split) -  pairs of models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P-P1IlEZv64"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gEzNY36Zv64"
      },
      "source": [
        "Individual Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y25HYUhGZv65"
      },
      "outputs": [],
      "source": [
        "# Evaluate each individual model at each epoch.\n",
        "individual_accuracies = []\n",
        "for model_info in tqdm(Model_sds, desc=\"Evaluating Individual Models\"):\n",
        "    model = get_model_from_sd(model_info['state_dict'], Base_model)\n",
        "    acc = eval_model_on_dataset(model, Test_loader)\n",
        "    individual_accuracies.append({\n",
        "        'name': model_info['name'],\n",
        "        'epoch': model_info['epoch'],\n",
        "        'accuracy': acc\n",
        "    })\n",
        "\n",
        "df_individual_acc = pd.DataFrame(individual_accuracies)\n",
        "print(\"✅ Individual model accuracies calculated.\")\n",
        "df_individual_acc.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw6h-ygbZv65"
      },
      "outputs": [],
      "source": [
        "# Merge individual accuracies into the main DataFrame\n",
        "df_results = df_results.merge(\n",
        "    df_individual_acc,\n",
        "    left_on=['model1_name', 'model1_epoch'],\n",
        "    right_on=['name', 'epoch']\n",
        ").rename(columns={'accuracy': 'model1_accuracy'}).drop(columns=['name', 'epoch'])\n",
        "\n",
        "df_results = df_results.merge(\n",
        "    df_individual_acc,\n",
        "    left_on=['model2_name', 'model2_epoch'],\n",
        "    right_on=['name', 'epoch']\n",
        ").rename(columns={'accuracy': 'model2_accuracy'}).drop(columns=['name', 'epoch'])\n",
        "\n",
        "# Calculate mean accuracy\n",
        "df_results['mean_accuracy'] = (df_results['model1_accuracy'] + df_results['model2_accuracy']) / 2\n",
        "print(\"✅ Individual accuracies merged and mean accuracy calculated.\")\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OZuQjrCZv65"
      },
      "source": [
        "Compute soup accuracy for each pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vr9WTBqZv65"
      },
      "outputs": [],
      "source": [
        "def create_soup(state_dicts, weights=None):\n",
        "    \"\"\"Create a model soup by averaging state dicts with given weights\"\"\"\n",
        "    if weights is None:\n",
        "        weights = [1.0 / len(state_dicts)] * len(state_dicts)\n",
        "\n",
        "    # Start with the first model weighted\n",
        "    soup_state_dict = {k: v.clone() * weights[0] for k, v in state_dicts[0].items()}\n",
        "\n",
        "    # Add remaining models\n",
        "    for i, state_dict in enumerate(state_dicts[1:], 1):\n",
        "        for k, v in state_dict.items():\n",
        "            soup_state_dict[k] += v.clone() * weights[i]\n",
        "\n",
        "    return soup_state_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSeKg60KZv65"
      },
      "outputs": [],
      "source": [
        "\n",
        "soup_accuracies = []\n",
        "model_map = {(m['name'], m['epoch']): m['state_dict'] for m in Model_sds}\n",
        "\n",
        "for _, row in tqdm(df_results.iterrows(), total=len(df_results), desc=\"Evaluating Soups\"):\n",
        "    sd1 = model_map[(row['model1_name'], row['model1_epoch'])]\n",
        "    sd2 = model_map[(row['model2_name'], row['model2_epoch'])]\n",
        "\n",
        "    # Create a soup model:  theta = 1/2(theta_i + theta_j)\n",
        "    soup_sd = create_soup([sd1, sd2])\n",
        "\n",
        "    # Evaluate the soup on test split\n",
        "    soup_model = get_model_from_sd(soup_sd, Base_model)\n",
        "    acc = eval_model_on_dataset(soup_model, Test_loader)\n",
        "    soup_accuracies.append(acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0sNJAWLZv6-"
      },
      "outputs": [],
      "source": [
        "df_results['soup_accuracy'] = soup_accuracies\n",
        "\n",
        "# Calculate mean accuracy and soup improvement\n",
        "df_results['soup_improvement'] = df_results['soup_accuracy'] - df_results['mean_accuracy']\n",
        "\n",
        "print(\"✅ Soup accuracies calculated and added to DataFrame.\")\n",
        "df_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all results"
      ],
      "metadata": {
        "id": "xUeCSSQmaVui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.to_csv(f'{storage_dir}/pair_soup_acc-pair_distance.csv', index=False)"
      ],
      "metadata": {
        "id": "ciaQsevhaMjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOriOLa1Zv6-"
      },
      "source": [
        "### Plot: soup accuracy(epoch)\n",
        "soup accuracy for any two models at same epoch.\n",
        "\n",
        "soup improvement above mean of each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmorsax1Zv6-"
      },
      "outputs": [],
      "source": [
        "# Filter for pairs of different models at the same epoch\n",
        "same_epoch_df = df_results[df_results['model1_epoch'] == df_results['model2_epoch']].copy()\n",
        "same_epoch_df['model_pair'] = same_epoch_df.apply(lambda r: f\"{r['model1_name']} vs {r['model2_name']}\", axis=1)\n",
        "\n",
        "# Create the plot\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "sns.lineplot(\n",
        "    data=same_epoch_df,\n",
        "    x='model1_epoch',\n",
        "    y='soup_accuracy',\n",
        "    hue='model_pair',\n",
        "    marker='o',\n",
        "    ax=ax\n",
        ")\n",
        "sns.lineplot(\n",
        "    data=same_epoch_df,\n",
        "    x='model1_epoch',\n",
        "    y='mean_accuracy',\n",
        "    hue='model_pair',\n",
        "    marker='x',\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "ax.set_title('Soup (o) and Mean (x) Accuracy of Model Pairs at Each Epoch', fontsize=16)\n",
        "ax.set_xlabel('Epoch', fontsize=12)\n",
        "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
        "ax.legend(title='Model Pairs', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(f'{storage_dir}/soup_accuracy_pairs_per_epoch.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaVIJILWZv6-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omv_6uulZv6-"
      },
      "source": [
        "## Relation between Soup performance and model L2 distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmEnGYq_Zv6-"
      },
      "source": [
        "### Plot: soup improvement vs L2 distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdlXOe3_Zv6-"
      },
      "outputs": [],
      "source": [
        "# Create the plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# Plot 1: Soup Accuracy vs. L2 Distance\n",
        "sns.scatterplot(\n",
        "    data=df_results,\n",
        "    x='l2_distance',\n",
        "    y='soup_accuracy',\n",
        "    ax=axes[0]\n",
        ")\n",
        "axes[0].set_title('Soup Accuracy vs. L2 Distance', fontsize=16)\n",
        "axes[0].set_xlabel('L2 Distance', fontsize=12)\n",
        "axes[0].set_ylabel('Soup Accuracy', fontsize=12)\n",
        "\n",
        "# Plot 2: Soup Improvement vs. L2 Distance\n",
        "sns.scatterplot(\n",
        "    data=df_results,\n",
        "    x='l2_distance',\n",
        "    y='soup_improvement',\n",
        "    ax=axes[1]\n",
        ")\n",
        "axes[1].set_title('Soup Improvement vs. L2 Distance', fontsize=16)\n",
        "axes[1].set_xlabel('L2 Distance', fontsize=12)\n",
        "axes[1].set_ylabel('Soup Improvement (Soup Acc - Mean Acc)', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(f'{storage_dir}/soup_accuracy_vs_pair_distance.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDP3-_Y6Zv6_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}